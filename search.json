[
  {
    "objectID": "Projects/Frameworks_Infrastructure_Development/basic_guidelines.html",
    "href": "Projects/Frameworks_Infrastructure_Development/basic_guidelines.html",
    "title": "Trusted AI - Basic Guidance for Framework Use",
    "section": "",
    "text": "‚ÄúGitOps‚Äù approach for software engineering\n\nSource Control\nCI/CD\nTesting!\n\nPython Centric Software Package Dependency Management\nLiterate programming nbdev, Jupyter (vscode), Integrated Documentation, Automatic generation of python modules and code\nData Centric AI Approach\nExperiment Management - DVC Experiment Management\nModel Management\n\nMLFlow\nDVC\nHugging Face Model Cards\nModel Evaluation Frameworks and Workflows\n\nIntegration of ‚ÄúSane Defaults‚Äù\n\nExample\n\nLicenses, Persistent Identifiers, Machine Readable Povenance, Vocabularies - OpenRAIL License - DOI for Datasets and Models - Hugging Face Model Cards\nAI Policy\n\nHuggingface AI Policy Interim Report\n\nThreat Modeling Securing the Future of Artificial Intelligence and Machine Learning at Microsoft\n\nMicrosoft Failure Modes in Machine Learning\nThreat Modeling AI/ML Systems and Dependencies",
    "crumbs": [
      "Projects",
      "Framework Infrastructure Development",
      "Trusted AI - Basic Guidance for Framework Use"
    ]
  },
  {
    "objectID": "Projects/Frameworks_Infrastructure_Development/basic_guidelines.html#basic-guidance",
    "href": "Projects/Frameworks_Infrastructure_Development/basic_guidelines.html#basic-guidance",
    "title": "Trusted AI - Basic Guidance for Framework Use",
    "section": "",
    "text": "‚ÄúGitOps‚Äù approach for software engineering\n\nSource Control\nCI/CD\nTesting!\n\nPython Centric Software Package Dependency Management\nLiterate programming nbdev, Jupyter (vscode), Integrated Documentation, Automatic generation of python modules and code\nData Centric AI Approach\nExperiment Management - DVC Experiment Management\nModel Management\n\nMLFlow\nDVC\nHugging Face Model Cards\nModel Evaluation Frameworks and Workflows\n\nIntegration of ‚ÄúSane Defaults‚Äù\n\nExample\n\nLicenses, Persistent Identifiers, Machine Readable Povenance, Vocabularies - OpenRAIL License - DOI for Datasets and Models - Hugging Face Model Cards\nAI Policy\n\nHuggingface AI Policy Interim Report\n\nThreat Modeling Securing the Future of Artificial Intelligence and Machine Learning at Microsoft\n\nMicrosoft Failure Modes in Machine Learning\nThreat Modeling AI/ML Systems and Dependencies",
    "crumbs": [
      "Projects",
      "Framework Infrastructure Development",
      "Trusted AI - Basic Guidance for Framework Use"
    ]
  },
  {
    "objectID": "Projects/Frameworks_Infrastructure_Development/howto.html",
    "href": "Projects/Frameworks_Infrastructure_Development/howto.html",
    "title": "Framework How To",
    "section": "",
    "text": "This page contains receipies on how to do tasks with the framework.\n\n\nThe framework requires Python to operate. See the section ?@sec-instaling=python if you don‚Äôt have a recent version of python installed. All projects are expected to be in a Git repository. Make sure [Git] is installed by typing git and (see section Section¬†1.4.2)\nIf you have expierence with Git, feel free to skip to the section ?@sec-trusted-storage.\nThere are also many guides online if you want more information, especially on using Git.\n\n\nEach project needs to live in a Git repository. If there is already a git repository set up, you can clone it to make a local copy on your computer. The exact details depend on where the repository is hosted, but the command will have the form\ngit clone https://github.com/nd-crane/moo.git\nWhere the git clone is the command the the URL will be the repository you want to make a local copy of.\n\n\n\nIf there isn‚Äôt a repository set up already one set up, make a new one by creating a new directory, and then using git init to initialize a new git repository. The command line steps are:\nmkdir my-ai-project\ncd my-ai-project\ngit init\nOnce you do this, you can then add some files and make the first commit. A good first file is the README file which should give a description of what this repository is. for each file or directory in your project, run git add for it to tell Git you want to track changes to that file.\n[edit README file]\ngit add README\ngit commit -m \"Initial Commit\"\n\n\n\nThis documentation assumes you are using PDM as a package maanger.\n\n\n\nSince the PDM pacakge manager is installed, we can use that to install DVC:\n$ pdm add dvc\nAdding packages to default dependencies: dvc\nüîí Lock successful\nChanges are written to pdm.lock.\nChanges are written to pyproject.toml.\nSynchronizing working set with lock file: 85 to add, 0 to update, 0 to remove\n\nüéâ All complete!\nConfigure DVC with remotes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDVC provides pipelines which are receipies used for each step in a training workflow. These are similar in spirit to a Makefile as used in software development. Pipelines let us record the exact sequence of commands as well as the command-line options used, for a training sequence. They also help new people get up to speed quickly on training and testing models.\n\n\n\nThis section contains more detailed descriptions for topics that need them.\n\n\nThe framework is implimented in python, and you need version 3.9 or greater. To find the version of python you have installed run python3 -V.\nThe steps needed to install Python depends on the operating system.\nAll this is placeholder. Perhaps there is an online tutorial elsewhere to explain this better?\n\nMacOS Easiest way is with Homebrew.\n$ brew install pyenv\n$ pyenv install 3.10.6\nLinux‚Ä¶how?\nWindows‚Ä¶no idea\n\n\n\n\nfill in.\n\n\n\nfill in."
  },
  {
    "objectID": "Projects/Frameworks_Infrastructure_Development/howto.html#installation-and-set-up",
    "href": "Projects/Frameworks_Infrastructure_Development/howto.html#installation-and-set-up",
    "title": "Framework How To",
    "section": "",
    "text": "The framework requires Python to operate. See the section ?@sec-instaling=python if you don‚Äôt have a recent version of python installed. All projects are expected to be in a Git repository. Make sure [Git] is installed by typing git and (see section Section¬†1.4.2)\nIf you have expierence with Git, feel free to skip to the section ?@sec-trusted-storage.\nThere are also many guides online if you want more information, especially on using Git.\n\n\nEach project needs to live in a Git repository. If there is already a git repository set up, you can clone it to make a local copy on your computer. The exact details depend on where the repository is hosted, but the command will have the form\ngit clone https://github.com/nd-crane/moo.git\nWhere the git clone is the command the the URL will be the repository you want to make a local copy of.\n\n\n\nIf there isn‚Äôt a repository set up already one set up, make a new one by creating a new directory, and then using git init to initialize a new git repository. The command line steps are:\nmkdir my-ai-project\ncd my-ai-project\ngit init\nOnce you do this, you can then add some files and make the first commit. A good first file is the README file which should give a description of what this repository is. for each file or directory in your project, run git add for it to tell Git you want to track changes to that file.\n[edit README file]\ngit add README\ngit commit -m \"Initial Commit\"\n\n\n\nThis documentation assumes you are using PDM as a package maanger.\n\n\n\nSince the PDM pacakge manager is installed, we can use that to install DVC:\n$ pdm add dvc\nAdding packages to default dependencies: dvc\nüîí Lock successful\nChanges are written to pdm.lock.\nChanges are written to pyproject.toml.\nSynchronizing working set with lock file: 85 to add, 0 to update, 0 to remove\n\nüéâ All complete!\nConfigure DVC with remotes."
  },
  {
    "objectID": "Projects/Frameworks_Infrastructure_Development/howto.html#trusted-process",
    "href": "Projects/Frameworks_Infrastructure_Development/howto.html#trusted-process",
    "title": "Framework How To",
    "section": "",
    "text": "DVC provides pipelines which are receipies used for each step in a training workflow. These are similar in spirit to a Makefile as used in software development. Pipelines let us record the exact sequence of commands as well as the command-line options used, for a training sequence. They also help new people get up to speed quickly on training and testing models."
  },
  {
    "objectID": "Projects/Frameworks_Infrastructure_Development/howto.html#how-tos",
    "href": "Projects/Frameworks_Infrastructure_Development/howto.html#how-tos",
    "title": "Framework How To",
    "section": "",
    "text": "This section contains more detailed descriptions for topics that need them.\n\n\nThe framework is implimented in python, and you need version 3.9 or greater. To find the version of python you have installed run python3 -V.\nThe steps needed to install Python depends on the operating system.\nAll this is placeholder. Perhaps there is an online tutorial elsewhere to explain this better?\n\nMacOS Easiest way is with Homebrew.\n$ brew install pyenv\n$ pyenv install 3.10.6\nLinux‚Ä¶how?\nWindows‚Ä¶no idea\n\n\n\n\nfill in.\n\n\n\nfill in."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Trusted AI",
    "section": "",
    "text": "AI/ML Attack Vectors and Threats Across the Lifecycle\n\n\nIn today‚Äôs rapidly evolving technological landscape, AI systems face numerous threats throughout their lifecycle. These threats range from data poisoning and Trojan insertion during the training phase to adversarial attacks and memory trojans during deployment. Such vulnerabilities can compromise the integrity, reliability, and trustworthiness of AI models. Moreover, inherent threats like lack of context, absence of correctness assurance, and unmeasured uncertainty further exacerbate the challenge. Addressing these issues is critical to developing AI systems that are secure, reliable, and trustworthy.",
    "crumbs": [
      "Trusted AI"
    ]
  },
  {
    "objectID": "index.html#the-challenge",
    "href": "index.html#the-challenge",
    "title": "Trusted AI",
    "section": "",
    "text": "AI/ML Attack Vectors and Threats Across the Lifecycle\n\n\nIn today‚Äôs rapidly evolving technological landscape, AI systems face numerous threats throughout their lifecycle. These threats range from data poisoning and Trojan insertion during the training phase to adversarial attacks and memory trojans during deployment. Such vulnerabilities can compromise the integrity, reliability, and trustworthiness of AI models. Moreover, inherent threats like lack of context, absence of correctness assurance, and unmeasured uncertainty further exacerbate the challenge. Addressing these issues is critical to developing AI systems that are secure, reliable, and trustworthy.",
    "crumbs": [
      "Trusted AI"
    ]
  },
  {
    "objectID": "index.html#the-objective",
    "href": "index.html#the-objective",
    "title": "Trusted AI",
    "section": "The Objective",
    "text": "The Objective\nTo overcome these challenges, our objective is to develop a comprehensive test and evaluation framework for AI that encompasses the following key areas:\n\nHuman Trust of AI/ML: Establishing and maintaining trust in AI systems by ensuring transparency, reliability, and ethical considerations.\nMeasures, Metrics, and Testing: Developing robust metrics and testing methodologies to evaluate AI performance, fairness, and safety.\nData Source Bias and Modularity: Identifying and mitigating biases in data sources and promoting modularity to enhance flexibility and robustness.\nCybersecurity + Risk Modeling: Integrating cybersecurity measures and risk modeling to protect AI systems from malicious attacks and vulnerabilities.\nDeveloping AI Workforce and Talent: Building a skilled workforce proficient in AI technologies and best practices to drive innovation and maintain high standards.",
    "crumbs": [
      "Trusted AI"
    ]
  },
  {
    "objectID": "index.html#technical-approach",
    "href": "index.html#technical-approach",
    "title": "Trusted AI",
    "section": "Technical Approach:",
    "text": "Technical Approach:\n\n\n\nApplication Trusted AI Principles within both the development and deployment lifecycle\n\n\nOur technical approach involves applying trusted AI principles throughout the AI system lifecycle, from development and use to analysis and re-design. This holistic approach ensures that trust is embedded in every phase of AI development and deployment.\n\nCircle of Trust For Trusted AI\nFor AI to succeed with trust as a core attribute, we need to develop a ‚Äúcircle of trust‚Äù where all AI activities follow best practices based on our 6 dimensions of trustworthiness:\n\nSafety and Robustness: Ensuring that AI systems operate safely and are resilient to attacks and failures. This involves rigorous testing and validation to detect and mitigate vulnerabilities.\nFairness: Eliminating biases and ensuring equitable outcomes for all users. Fairness requires continuous monitoring and adjustments to avoid discrimination and ensure inclusivity.\nPrivacy: Protecting user data and maintaining confidentiality throughout the AI lifecycle. Privacy measures include data anonymization, secure data storage, and compliance with data protection regulations.\nSustainability: Maintaining AI operations over time without degradation in performance. Sustainability encompasses efficient resource utilization, long-term maintenance, and adaptability to evolving requirements.\nAccountability: Clearly defining responsibility and control over AI actions. Accountability involves transparent decision-making processes, audit trails, and mechanisms for addressing failures or unintended consequences.\nExplainability: Providing clarity on how decisions are made by the AI. Explainability helps users understand AI behavior, builds trust, and facilitates debugging and improvement of AI models.\n\nBy integrating these dimensions into the AI system lifecycle‚Äîspanning development, use, analysis, and re-design‚Äîwe ensure that AI systems are trustworthy, reliable, and aligned with ethical standards.",
    "crumbs": [
      "Trusted AI"
    ]
  },
  {
    "objectID": "index.html#trusted-ai-projects",
    "href": "index.html#trusted-ai-projects",
    "title": "Trusted AI",
    "section": "Trusted AI Projects",
    "text": "Trusted AI Projects\nThe nd-crane GitHub Organization hosts repositories for the following TAI projects.\n\nHuman-machine Pairing for Trustworthy AI: (Adam Czajka)\nDevelop a framework for human-machine supervision cycle, with its validation in the realm of computer vision and security areas, which will allow both sides ‚Äì humans and AI systems ‚Äìto interact, learn from each other and, as an overarching goal, increase the trustworthiness in AI systems.\n\n\nTrust and Verifiability in AI: (Adam Czajka)\nDevelop an approach for testing the verifiability of AI, which is designed to primarily work with black-box models, but will support white-box testing also. To support today‚Äôs warfighter, where solutions are based on AI models in embedded systems, effective black box tools are needed to help establish the verifiability of the AI, even when solutions are proprietary, and neither the training data or the algorithms are available.\n\n\nStatistical Analysis and Measurement of Neural Networks: (Chris Sweet)\nInvestigate and develop a statistical and computational framework to test, analyze, and enhance Neural Network models to help identify and alleviate potential failures and weaknesses, including those that occur naturally and those deliberately created by adversaries.\n\n\nKnowledge Representation and Engineering: (Paul Brenner)\nIdentifying the complex causes of potential mission or weapon system failure (or success) and determining effective responses to preventing (or ensuring) such requires leveraging best in class data analytics techniques on rapidly growing, but often poorly structured, data. To facilitate this approach, natural language processing (NLP) and related machine learning tools such as knowledge graphs can be harnessed to gain insight and answer these critical questions. Further information can be found at https://nd-crane.github.io.\n\n\nFramework Infrastructure Development: (Charles Vardeman II)\nThe T&E Web UI and Framework provides a graphical user interface and backend framework for connecting the various components and toolboxes together into a single coherent system. This component essentially provides a sandbox for Crane T&E to interact with, to define information surrounding an AI instance, to document and define the T&E activities undertaken, and to help automate testing. Further information can be found at https://nd-crane.github.io and https://la3d.github.io/nuggets/posts/frameworks-reflection/.",
    "crumbs": [
      "Trusted AI"
    ]
  },
  {
    "objectID": "Projects/Frameworks_Infrastructure_Development/components.html",
    "href": "Projects/Frameworks_Infrastructure_Development/components.html",
    "title": "Framework Components",
    "section": "",
    "text": "The framework consists of several tools that work together to promote Trust. This document describes the specific roles needed and how each role fits together. While we have selected a group of software tools that work together, it is possible to swap out components so long as the functions required in each role are achieved. For example, different tools may be required when applying the framework to a legacy envrionment or code base.\nFor a more conceptual discussion on Trust in AI see the framework description. And for a more hands-on walkthrough, see the [tutorial].\nThe roles are\n\nSource Code Control and Data Control (Control over source code and data)\nDependency Management (Control over third-party software libraries)\nBuild and Training Management (Control over the training process)\n\n\n\nThe first component of the framework is software source control. Source control tracks changes to the model code over time. Tracking code changes is essential to tracking the provanence of each trained model. Source control also makes it possible to coordinate changes between a team, and to develop automated workflows.\nThe current state of the art is a distributed version control system, such as Git or Mercurial. We recommend Git if you do not already have a source control system in place. It is very common, and while there are public platforms providing Git hosting, that is not essential and private hosting is easy to set up.\nIn addition to model code, it is also important to track training data for model provanance. Since training data is often very large, it is not a good fit for source control systems, A storage overlay system is software that integrates with the version control systems to handle large files. These overlays will track the versions of the data files in the source code repository, but will store the large files in a separate location. We recommend using the [Data Version Control] (DVC) system since in addition to providing the storage overlay, it can also perform some of the other roles we will discuss.\nThe features as part of the overlay, one can add files, remove files, put things in an independant storage location.\nTrack the data used for each training run. Make sure every member of a team is using the same data.\n\n\n\nAll software these days are built on third-party libraries, and managing these libraries is essential for any serious project. One needs to track the versions of the libraries used, as well as any libraries they use in turn. This is a complicated situation, and most programming languages and frameworks have a tool for this dependicy management. Using a dependicy manager is essential to track provance for any output files.\nSince most nueral network models use Python, we recommend using PDM for Python dependecy management. The files generated by the dependcy manager should then be tracked in the source control system.\nAside: Python dependecy management is not simple to figure out. There are two general mechansisms for providing the isolation needed between packages, the [virtualenv] method and the [PEP 582] method. The PEP 582 method has many nice benifits, such as self contained project directoriesr, but support for it is not quite mature yet. So we recommend using the virtualenv way. Fortunately PDM can handle both, and defaults to using virtualenv. (To configure PDM to use the PEP-582 mode, use the command\npdm config --local python.use_venv False¬¨\n)\n\n\n\nAs a machine learning project evolves, the training steps will envariably grow and the commands will get more complicated. But even before then, it is important to record the commands used for each training step. A build tool is essential for reproducible builds and model training. Such a tool allows one to save all the steps in a file and store it in the source control system. A build tool will track the commands that are needed to do each task as well as tracking which tasks depend on others, so that time is not wasted reprocessing things that have not changed.\nWe recommend using DVC for this. using its pipeline feature.\nYou will want a system that lets you organize your commands into independant stages and will track the inputs and outputs of each stage so that only pieces that have changed will be rurun.\n\n\n\nAs models become more complicated, there will usually be parameters that can be adjusted between model trainings. Since parameters tune a model without modifying its code, their settings are not tracked in the source control system. An experiment manager will let us formally declare the parameters and track their settings in model trainings.\nDVC is our recommendation for this since experiments are integrated with its pipeline build tool. In addition DVC can run parameter sweeps for model tuning, and can plot output metrics for each model. Additionally, these output files can be saved back to the independent data storage location for documentation and sharing with others.",
    "crumbs": [
      "Projects",
      "Framework Infrastructure Development",
      "Framework Components"
    ]
  },
  {
    "objectID": "Projects/Frameworks_Infrastructure_Development/components.html#source-code-control-and-data-control",
    "href": "Projects/Frameworks_Infrastructure_Development/components.html#source-code-control-and-data-control",
    "title": "Framework Components",
    "section": "",
    "text": "The first component of the framework is software source control. Source control tracks changes to the model code over time. Tracking code changes is essential to tracking the provanence of each trained model. Source control also makes it possible to coordinate changes between a team, and to develop automated workflows.\nThe current state of the art is a distributed version control system, such as Git or Mercurial. We recommend Git if you do not already have a source control system in place. It is very common, and while there are public platforms providing Git hosting, that is not essential and private hosting is easy to set up.\nIn addition to model code, it is also important to track training data for model provanance. Since training data is often very large, it is not a good fit for source control systems, A storage overlay system is software that integrates with the version control systems to handle large files. These overlays will track the versions of the data files in the source code repository, but will store the large files in a separate location. We recommend using the [Data Version Control] (DVC) system since in addition to providing the storage overlay, it can also perform some of the other roles we will discuss.\nThe features as part of the overlay, one can add files, remove files, put things in an independant storage location.\nTrack the data used for each training run. Make sure every member of a team is using the same data.",
    "crumbs": [
      "Projects",
      "Framework Infrastructure Development",
      "Framework Components"
    ]
  },
  {
    "objectID": "Projects/Frameworks_Infrastructure_Development/components.html#dependency-management",
    "href": "Projects/Frameworks_Infrastructure_Development/components.html#dependency-management",
    "title": "Framework Components",
    "section": "",
    "text": "All software these days are built on third-party libraries, and managing these libraries is essential for any serious project. One needs to track the versions of the libraries used, as well as any libraries they use in turn. This is a complicated situation, and most programming languages and frameworks have a tool for this dependicy management. Using a dependicy manager is essential to track provance for any output files.\nSince most nueral network models use Python, we recommend using PDM for Python dependecy management. The files generated by the dependcy manager should then be tracked in the source control system.\nAside: Python dependecy management is not simple to figure out. There are two general mechansisms for providing the isolation needed between packages, the [virtualenv] method and the [PEP 582] method. The PEP 582 method has many nice benifits, such as self contained project directoriesr, but support for it is not quite mature yet. So we recommend using the virtualenv way. Fortunately PDM can handle both, and defaults to using virtualenv. (To configure PDM to use the PEP-582 mode, use the command\npdm config --local python.use_venv False¬¨\n)",
    "crumbs": [
      "Projects",
      "Framework Infrastructure Development",
      "Framework Components"
    ]
  },
  {
    "objectID": "Projects/Frameworks_Infrastructure_Development/components.html#build-tools",
    "href": "Projects/Frameworks_Infrastructure_Development/components.html#build-tools",
    "title": "Framework Components",
    "section": "",
    "text": "As a machine learning project evolves, the training steps will envariably grow and the commands will get more complicated. But even before then, it is important to record the commands used for each training step. A build tool is essential for reproducible builds and model training. Such a tool allows one to save all the steps in a file and store it in the source control system. A build tool will track the commands that are needed to do each task as well as tracking which tasks depend on others, so that time is not wasted reprocessing things that have not changed.\nWe recommend using DVC for this. using its pipeline feature.\nYou will want a system that lets you organize your commands into independant stages and will track the inputs and outputs of each stage so that only pieces that have changed will be rurun.",
    "crumbs": [
      "Projects",
      "Framework Infrastructure Development",
      "Framework Components"
    ]
  },
  {
    "objectID": "Projects/Frameworks_Infrastructure_Development/components.html#experiment-management",
    "href": "Projects/Frameworks_Infrastructure_Development/components.html#experiment-management",
    "title": "Framework Components",
    "section": "",
    "text": "As models become more complicated, there will usually be parameters that can be adjusted between model trainings. Since parameters tune a model without modifying its code, their settings are not tracked in the source control system. An experiment manager will let us formally declare the parameters and track their settings in model trainings.\nDVC is our recommendation for this since experiments are integrated with its pipeline build tool. In addition DVC can run parameter sweeps for model tuning, and can plot output metrics for each model. Additionally, these output files can be saved back to the independent data storage location for documentation and sharing with others.",
    "crumbs": [
      "Projects",
      "Framework Infrastructure Development",
      "Framework Components"
    ]
  },
  {
    "objectID": "Projects/Frameworks_Infrastructure_Development/framework.html",
    "href": "Projects/Frameworks_Infrastructure_Development/framework.html",
    "title": "Trusted AI - Frameworks for Enabling Trusted AI",
    "section": "",
    "text": "One of the motivations behind the TrustedAI research consortium is to translate cutting edge Artificial Intelligence (AI) and Machine Learning research into conceptual frameworks and concrete tools to facilitate the trusted adoption of AI based technologies by the United States Department of Defense using the United States Navy as a concrete test case. The Frameworks Project was intended to be a conduit for integrating both a set of conceptual best practices for use when developing, evaluating and deploying AI technologies, and a set of software tools that enable the conceptual frameworks. To facilitate adoption of both ‚Äúframeworks‚Äù, the project looked to successful software engineering tools and principles such as Continuous Integration/Continuous Delivery (CI/CD) tools that widely adopt software version control systems such as git as well as software repositories like GitHub. This ‚ÄúGitOps‚Äù approach is being increasingly adopted in private sector software projects and facilitates team based software development, automated testing and deployment, and issue tracking, collaboration and compliance. Given that AI software is currently implemented in classical ‚ÄúSoftware 1.0‚Äù written in languages like Python, C++, etc, starting with trusted engineering of the traditional software stack is foundational to the practice of TrustedAI. We also recognize that there is a ‚ÄúAI software‚Äù lifecycle akin to the CI/CD lifecycle in traditional software engineering that correspond to the ‚Äúouter ring‚Äù in Figure¬†1 that is Development of AI software, Use of that software in real world deployments, Analysis of the AI software performance i.e.¬†‚ÄîTrustworthiness‚Äî, and Re-design or Re-training of AI to better meet the underlying system requirements. The GitOps approach also has the advantage of facilitate the tracking of provenance information that, at the most general level, Trustworthy AI should have the attributes of being lawful, ethical and robust throughout the AI System Lifecycle (Smuha, n.d.).\n\n\n\n\n\n\nFigure¬†1: Conceptual Framework Principles for TrustedAI in the AI System Lifecycle\n\n\n\nHowever, AI software is yet fundamentally different from traditional software implementations because the software behaviors are learned behaviors, not behavior encoded by a programmer in the program through some algorithm. This new type of software has been termed ‚ÄúSoftware 2.0‚Äù by some of the AI community and is based on the observation that the programming paradigm is being fundamentally altered from implementation of algorithms in the form of code to labeling, curating and engineering data that will be used to define software behavior that integrates into a Chain of Trust (Toreini et al. 2020) along the ML Pipeline. This observation of the critical nature of training data in the AI based software development process has lead the ‚ÄúData-centric‚Äù AI discipline that aims to create principles and best practices behind systematically engineering data used to construct AI software. Data-centric AI also reflects the United States Department of Defense Data Strategy (‚ÄúDoD Data Strategy: Unleashing Data to Advance the National Defense Strategy‚Äù 2020) where data should have the goals of being Visible, Accessible, Understandable, Linked, Trustworthy, Interoperable, and Secure as illustrated in Figure¬†2.\nThe Data-centric AI principles form the second foundational pillar for the frameworks in exploring and adopting tools that facilitate this ‚Äúdata and knowledge engineering process‚Äù through Data Collection, Data Preparation and Feature Extraction. We believe that Data-centric AI can benifit from knowledge enginnering, where a ‚Äúformal‚Äù conceptual layer that encodes a specification of the human conceptualization with respect to the data is used to give AI deeper context of data meaning (Ilievski et al. 2021). At each stage of the ML Pipeline each of the Six Dimensions of Trust as identified by (Liu et al. 2022) can be applied. In other words, each step of the ML Pipeline should be engineered with respect to Explainability, Safety & Robustness, Non-discrimination & Fairness, Accountability and Auditability, Environmental Well-being and Privacy.\n\nThe goal of the Framework tools is to integrate existing software engineering tools and best practices as well as Trusted AI Project outputs with respect to each of these six dimension‚Äôs of trust that can be used at each stage of the ML Pipeline to create a ‚ÄúToolbox‚Äù for practitioners and evaluators to utilize in enabling each of the dimensions in a modular set of software frameworks. This Framework Infrastructure connects the various components and toolboxes being developed for the Trusted AI project together into a single coherent system. The system provides standardized methods for storage and version control of code, data and documentation. There is a minimum viable vocabulary to describe experiments, as well as metadata and storage criterion for model/data outputs such as Neural Network weights. Preference is given to Open Source Software and Tools that can either be integrated into the Framework or modified by our software development teams with possible contribution back to their parent project.",
    "crumbs": [
      "Projects",
      "Framework Infrastructure Development",
      "Trusted AI - Frameworks for Enabling Trusted AI"
    ]
  },
  {
    "objectID": "Projects/Frameworks_Infrastructure_Development/framework.html#introduction",
    "href": "Projects/Frameworks_Infrastructure_Development/framework.html#introduction",
    "title": "Trusted AI - Frameworks for Enabling Trusted AI",
    "section": "",
    "text": "One of the motivations behind the TrustedAI research consortium is to translate cutting edge Artificial Intelligence (AI) and Machine Learning research into conceptual frameworks and concrete tools to facilitate the trusted adoption of AI based technologies by the United States Department of Defense using the United States Navy as a concrete test case. The Frameworks Project was intended to be a conduit for integrating both a set of conceptual best practices for use when developing, evaluating and deploying AI technologies, and a set of software tools that enable the conceptual frameworks. To facilitate adoption of both ‚Äúframeworks‚Äù, the project looked to successful software engineering tools and principles such as Continuous Integration/Continuous Delivery (CI/CD) tools that widely adopt software version control systems such as git as well as software repositories like GitHub. This ‚ÄúGitOps‚Äù approach is being increasingly adopted in private sector software projects and facilitates team based software development, automated testing and deployment, and issue tracking, collaboration and compliance. Given that AI software is currently implemented in classical ‚ÄúSoftware 1.0‚Äù written in languages like Python, C++, etc, starting with trusted engineering of the traditional software stack is foundational to the practice of TrustedAI. We also recognize that there is a ‚ÄúAI software‚Äù lifecycle akin to the CI/CD lifecycle in traditional software engineering that correspond to the ‚Äúouter ring‚Äù in Figure¬†1 that is Development of AI software, Use of that software in real world deployments, Analysis of the AI software performance i.e.¬†‚ÄîTrustworthiness‚Äî, and Re-design or Re-training of AI to better meet the underlying system requirements. The GitOps approach also has the advantage of facilitate the tracking of provenance information that, at the most general level, Trustworthy AI should have the attributes of being lawful, ethical and robust throughout the AI System Lifecycle (Smuha, n.d.).\n\n\n\n\n\n\nFigure¬†1: Conceptual Framework Principles for TrustedAI in the AI System Lifecycle\n\n\n\nHowever, AI software is yet fundamentally different from traditional software implementations because the software behaviors are learned behaviors, not behavior encoded by a programmer in the program through some algorithm. This new type of software has been termed ‚ÄúSoftware 2.0‚Äù by some of the AI community and is based on the observation that the programming paradigm is being fundamentally altered from implementation of algorithms in the form of code to labeling, curating and engineering data that will be used to define software behavior that integrates into a Chain of Trust (Toreini et al. 2020) along the ML Pipeline. This observation of the critical nature of training data in the AI based software development process has lead the ‚ÄúData-centric‚Äù AI discipline that aims to create principles and best practices behind systematically engineering data used to construct AI software. Data-centric AI also reflects the United States Department of Defense Data Strategy (‚ÄúDoD Data Strategy: Unleashing Data to Advance the National Defense Strategy‚Äù 2020) where data should have the goals of being Visible, Accessible, Understandable, Linked, Trustworthy, Interoperable, and Secure as illustrated in Figure¬†2.\nThe Data-centric AI principles form the second foundational pillar for the frameworks in exploring and adopting tools that facilitate this ‚Äúdata and knowledge engineering process‚Äù through Data Collection, Data Preparation and Feature Extraction. We believe that Data-centric AI can benifit from knowledge enginnering, where a ‚Äúformal‚Äù conceptual layer that encodes a specification of the human conceptualization with respect to the data is used to give AI deeper context of data meaning (Ilievski et al. 2021). At each stage of the ML Pipeline each of the Six Dimensions of Trust as identified by (Liu et al. 2022) can be applied. In other words, each step of the ML Pipeline should be engineered with respect to Explainability, Safety & Robustness, Non-discrimination & Fairness, Accountability and Auditability, Environmental Well-being and Privacy.\n\nThe goal of the Framework tools is to integrate existing software engineering tools and best practices as well as Trusted AI Project outputs with respect to each of these six dimension‚Äôs of trust that can be used at each stage of the ML Pipeline to create a ‚ÄúToolbox‚Äù for practitioners and evaluators to utilize in enabling each of the dimensions in a modular set of software frameworks. This Framework Infrastructure connects the various components and toolboxes being developed for the Trusted AI project together into a single coherent system. The system provides standardized methods for storage and version control of code, data and documentation. There is a minimum viable vocabulary to describe experiments, as well as metadata and storage criterion for model/data outputs such as Neural Network weights. Preference is given to Open Source Software and Tools that can either be integrated into the Framework or modified by our software development teams with possible contribution back to their parent project.",
    "crumbs": [
      "Projects",
      "Framework Infrastructure Development",
      "Trusted AI - Frameworks for Enabling Trusted AI"
    ]
  },
  {
    "objectID": "Projects/Frameworks_Infrastructure_Development/framework.html#united-states-department-of-defense-and-trusted-ai-frameworks",
    "href": "Projects/Frameworks_Infrastructure_Development/framework.html#united-states-department-of-defense-and-trusted-ai-frameworks",
    "title": "Trusted AI - Frameworks for Enabling Trusted AI",
    "section": "2 United States Department of Defense and Trusted AI Frameworks",
    "text": "2 United States Department of Defense and Trusted AI Frameworks\nPart of the Trusted AI Frameworks mission is to help operationalize the DOD vision with respect to ‚ÄúImplimenting Responsible Artificial Intelligence in the Department of Defense‚Äù (Hicks 2021) that outlines the core Ethical principles that goveren the ‚Äú‚Ä¶design, development, deployment, and use of Al capabilities‚Äù for the DoD. These core ethical principles demand that AI be Responsible, Equitable, Traceable, Reliable and Governable (Joint Center for Artificial Intelligence 2020). According the guidelines, the foundational tenets for any Responsible AI (RAI) implimentenation will include RAI Governance, Warfighter Trust, AI Product and Acquisition Lifecycle, Requirements Validation, Responsible AI Ecosystem, and AI Workforce. These RAI tenets map to elements of the Conceptual Framework Principles for TrustedAI in the AI System Lifecycle in Figure¬†1, particularly in establishing Warfighter Trust and informing the AI Product and Acquisition Lifecycle best practices. The RAI guidelines deeply inform both the TrustedAI project as a whole and the TrustedAI Frameworks project particularly in building tools and educating the AI Workforce in both the core ethical principles and RAI tenets. A Data-centric approach is critical implimenting the RAI tenets is forms the core of the Trusted AI Framworks centering on the DoD Data Strategy as both a principle for ‚Äúlong-term data competency grounded in high-quality training datasets‚Äù (‚ÄúDoD Data Strategy: Unleashing Data to Advance the National Defense Strategy‚Äù 2020), but also to structure provenance information relative to the AI lifecycle and its connection to RAI. This data strategy is represented in Figure¬†2 and is a core compentency the TrustedAI set of Frameworks will help to enable.\n\n\n\n\n\n\nFigure¬†2: DoD Data Strategy Framework from ‚ÄúDoD Data Strategy: Unleashing Data to Advance the National Defense Strategy‚Äù (2020)\n\n\n\n\n2.1 Informing DoD Acquisition of AI Technologies\nPart of the motivation for the TrustedAI effort was to help inform the procurment and evaluation of AI based technologies by the United States Navy in concert with the RAI and other Department of Defense guidelines. As outlined in (Bowne and McMartin 2022), this will likely require changes in how the DoD procures and evaluates AI technologies as well as changing the legal landscape in terms of licensing and contracting with AI technology suppliers. Changes in this landscape will likely require suppliers to adopt new tooling for provenance capture with respect to data and model to ensure compliance with RAI guidelines. Luckly, as technology development moves toward the ‚ÄúSoftware 2.0‚Äù paradigm, these considerations can be built into the software development infrastructure to help ensure better adherence to the RAI tenets. The Trusted AI Frameworks aim to help inform a preliminary iteration of what these tools and best practices for technology suppliers could look like.",
    "crumbs": [
      "Projects",
      "Framework Infrastructure Development",
      "Trusted AI - Frameworks for Enabling Trusted AI"
    ]
  },
  {
    "objectID": "Projects/Frameworks_Infrastructure_Development/framework.html#leveraging-ai-community-tools",
    "href": "Projects/Frameworks_Infrastructure_Development/framework.html#leveraging-ai-community-tools",
    "title": "Trusted AI - Frameworks for Enabling Trusted AI",
    "section": "3 Leveraging AI Community Tools",
    "text": "3 Leveraging AI Community Tools\nAs outlined in the Introduction, part of the goal for the Frameworks project was to identify tools that could be leveraged as part of the AI software lifecycle that provided functionality with respect to the dimensions of trust. One attribute of trust for (open source) software tools is that there is an active community of developers maintaining the tool and providing a broader community of support for using the tools. After initial consultation with the stakeholders and project members, it was decided that Git would be the primary software source control tool and Github as the initial source code repository. Github is provides a commercial software source control and collaborative development environment that is friendly to open source developers in providing free public/private repositories. Other options for git source repositories exist such as Gitlab and the self-hosted Gitlab Community Edition that provide migration paths for git-centric source code repositories if security and privacy requirements require self-hosted options.\nOne way to enable Data-Centric AI (Liang et al. 2022) within the ‚ÄúGitOps ecosystem‚Äù is through the use of Data Version Control (DVC) that facilitates tracking of ML Models and Datasets by direct integration into the git version control system. DVC allows data to be versioned and then stored outside git repositories in a wide variety of data storage systems facilitating the use of larger datasets and storage systems than could be otherwise accommodated by git or through the use of Git Large File Storage. DVC also has tools to help facilitate experiment reproducibility by maintaining information about input data, configuration and code used to run an experiment all within the git source control environment. The general methodology for the core Trusted AI Framework is that all model code, data preparation code, and training workflows are tracked within a Git Repository. Training data, generated models, and analytics are tracked by DVC and enabeling a potentially separate data storage environment using one of the DVC Supported storage types.",
    "crumbs": [
      "Projects",
      "Framework Infrastructure Development",
      "Trusted AI - Frameworks for Enabling Trusted AI"
    ]
  },
  {
    "objectID": "Projects/Frameworks_Infrastructure_Development/framework.html#tai-projects-as-framework-testbeds",
    "href": "Projects/Frameworks_Infrastructure_Development/framework.html#tai-projects-as-framework-testbeds",
    "title": "Trusted AI - Frameworks for Enabling Trusted AI",
    "section": "4 TAI Projects as Framework Testbeds",
    "text": "4 TAI Projects as Framework Testbeds\nSeveral TrustedAI projects were used as use case/test environments for the Core Framework tools during the first year of TrustedAI. These projects provided different data/machine learning model requirements to explore how effective the tools and methodology are in\n\nKnowledge Graph Construction using Natural Language Processing Tools",
    "crumbs": [
      "Projects",
      "Framework Infrastructure Development",
      "Trusted AI - Frameworks for Enabling Trusted AI"
    ]
  },
  {
    "objectID": "Projects/Frameworks_Infrastructure_Development/tutorial.html",
    "href": "Projects/Frameworks_Infrastructure_Development/tutorial.html",
    "title": "Trusted AI Framework Tutorial",
    "section": "",
    "text": "This tutorial will guide you with setting up the framework and running a simple model. Begin by installing Git (see ?@sec-installing-git) and if needed installing Python (?@sec-installing-python). Using the command line, type the following to create a new directory for the new project:\nmkdir ai-tutorial\nWe will now go into the new directory and initialize PDM. First, tell PDM to use the PEP 582 method of storing dependencies. (Don‚Äôt type the $ character‚Äîit is used to mark the commands typed into the terminal,)\n$ cd ai-tutorial\n$ pdm config --local python.use_venv False\nAnd now have PDM generate an initital manifest.\n$ pdm init\nCreating a pyproject.toml for PDM...\nPlease enter the Python interpreter to use\n0. /opt/homebrew/opt/python@3.10/bin/python3.10 (3.10)\n1. /opt/homebrew/opt/python@3.10/bin/python3.10 (3.10)\n2. /opt/homebrew/opt/python@3.9/bin/python3.9 (3.9)\n3. /Users/dbrower/.pyenv/versions/3.9.10/bin/python3.9 (3.9)\n4. /Library/Developer/CommandLineTools/usr/bin/python3 (3.9)\nPlease select (0): 0\nUsing Python interpreter: /opt/homebrew/opt/python@3.10/bin/python3.10 (3.10)\nWould you like to create a virtualenv with /opt/homebrew/opt/python@3.10/bin/python3.10? [y/n] (y): y\nVirtualenv is created successfully at ./ai-tutorial/.venv\nIs the project a library that will be uploaded to PyPI [y/n] (n): n\nLicense(SPDX name) (MIT):\nAuthor name (Jane Doe):\nAuthor email (jdoe@example.com):\nPython requires('*' to allow any) (&gt;=3.10):\nChanges are written to pyproject.toml.\nPDM will give a few prompts. For the python version, use the one that you had previously installed‚Äîusually the first option. So chose 0. Type y to set up a virtualenv. And then type n since will not be a PyPI library. For the next four options press Enter to choose the defaults. At the end, PDM will create a file named pyproject.toml.\nLets create a new git repository for this model.\n$ git init\nInitialized empty Git repository in /ai-tutorial/.git/\n$ git add .pdm.toml pyproject.toml\n$ git commit -m \"Initial Commit\"\n[main (root-commit) 7fc208a] Initial Commit\n 1 file changed, 14 insertions(+)\n create mode 100644 .pdm.toml\n create mode 100644 pyproject.toml\n```\nNow lets create a machine learning model. For this tutorial, we will use the PyTorch beginner quickstart project.\nDownload the quickstart source code from GitHub:\ncurl https://raw.githubusercontent.com/pytorch/tutorials/master/beginner_source/basics/quickstart_tutorial.py &gt; quickstart_tutorial.py\nThis code uses some external Python models, so we need to tell PDM to fetch them:\npdm add torch torchvision\nNow add DVC.\n$ pdm add dvc\n$ pdm run dvc init\nLets commit our changes to the local Git repository\n$ git add .dvc .dvcignore quickstart_tutorial.py\n$ git commit -m \"Adding DVC and model code\"\nAnd now lets run the model. The first part of the model will download the training data. It will then run 5 epochs of the training cycle.\npdm run python quickstart_tutorial",
    "crumbs": [
      "Projects",
      "Framework Infrastructure Development",
      "Trusted AI Framework Tutorial"
    ]
  },
  {
    "objectID": "Projects/Frameworks_Infrastructure_Development/tutorial.html#james-example",
    "href": "Projects/Frameworks_Infrastructure_Development/tutorial.html#james-example",
    "title": "Trusted AI Framework Tutorial",
    "section": "2.1 james example",
    "text": "2.1 james example\nPDM should be installed as described in the Installation instructions.",
    "crumbs": [
      "Projects",
      "Framework Infrastructure Development",
      "Trusted AI Framework Tutorial"
    ]
  },
  {
    "objectID": "Projects/Frameworks_Infrastructure_Development/tutorial.html#data-version-control-dvc",
    "href": "Projects/Frameworks_Infrastructure_Development/tutorial.html#data-version-control-dvc",
    "title": "Trusted AI Framework Tutorial",
    "section": "2.2 Data Version Control (DVC)",
    "text": "2.2 Data Version Control (DVC)\n\n2.2.1 Program Execution\nThe first place to start using DVC is to wrap the execution of the model with DVC, allowing dvc to track specified dependencies, parameters and outputs.\nAt the moment, our primary dependency is the python file itself, and we should also include the pyproject.toml and pdm.lock files as our results might change if we change something about our dependencies.\npdm run dvc stage add -n tutorial -d quickstart_tutorial.py -d pyproject.toml -d pdm.lock python quickstart_tutorial.py\n\n\n2.2.2 Reproducing Results\nDVC allows us to reproduce the results by running pdm run dvc repro. This will execute the code and track the information we have told DVC to track. It stores this information in te dvc.lock file. If we run the repro command again, DVC will tell us that nothing changed and so we don‚Äôt need to run anything.",
    "crumbs": [
      "Projects",
      "Framework Infrastructure Development",
      "Trusted AI Framework Tutorial"
    ]
  },
  {
    "objectID": "Projects/Frameworks_Infrastructure_Development/tutorial.html#sec-trusted-storage",
    "href": "Projects/Frameworks_Infrastructure_Development/tutorial.html#sec-trusted-storage",
    "title": "Trusted AI Framework Tutorial",
    "section": "2.3 Trusted Storage",
    "text": "2.3 Trusted Storage\nTrusted Storage refers to the steps needed to keep intellectual control over an AI model. This means that you can point to the exact code used to construct each version of the model, as well as the exact training data used, how the data was preprocessed, and the parameters used for each expeirment. The foundation is Git as a version control system. On top of that we add a storage overlay provided by the Data Version Control (DVC) project. DVC provides a way to track file versions in Git, but store the contents elseware, which is useful since training data and models tend to be large files, something Git is not optimized for.\nDVC also provides pipelines which are essentailly receipies used for each step in a training workflow. These are similar in spirit to a Makefile as used in software development.",
    "crumbs": [
      "Projects",
      "Framework Infrastructure Development",
      "Trusted AI Framework Tutorial"
    ]
  },
  {
    "objectID": "Projects/Frameworks_Infrastructure_Development/tutorial.html#general-workflow-steps",
    "href": "Projects/Frameworks_Infrastructure_Development/tutorial.html#general-workflow-steps",
    "title": "Trusted AI Framework Tutorial",
    "section": "2.4 General Workflow Steps",
    "text": "2.4 General Workflow Steps\nOnce a project is set up, the general workflow is:\n\nAdd or update data files, and download existing data files\nUpdate code, run experiments\nCheck in changes to both code and data\n\nWe will now go through each step in detail.",
    "crumbs": [
      "Projects",
      "Framework Infrastructure Development",
      "Trusted AI Framework Tutorial"
    ]
  },
  {
    "objectID": "Projects/Knowledge_Representation_and_Engineering/kg_nlp.html",
    "href": "Projects/Knowledge_Representation_and_Engineering/kg_nlp.html",
    "title": "Trusted AI - Natural Language Processing and Knowledge Graphs for Naval Systems Intelligence",
    "section": "",
    "text": "Identifying the complex causes of potential mission or weapon system failure (or success) and determining effective responses to preventing (or ensuring) such requires leveraging best in class machine learning techniques on rapidly growing, but often poorly structured, data. While the tools available for data science continue to evolve, there remain significant challenges for teams of decision makers trying to wrangle insight from the large and complex data accessible to them. The processes of reviewing, labeling, and classifying massive amounts of information takes extensive time, money, and human power. Fortunately, recent advances in natural language processing (NLP) and related machine learning tools such as knowledge graphs (KG) can be harnessed to gain insight and answer these critical questions. Growing availability of both open source and commercial NLP tools has made it easier for experimentation but also easier to use machine learning algorithms as ‚Äúblack box‚Äù tools in which data is blindly input and tool parameters tweaked to obtain the highest accuracy score. This ‚Äúblack box‚Äù approach introduces uncertainty (untrusted AI) which could lead to global inefficiencies at best and unexpected miss-classifications at worst. Our framework is being developed with the larger set of Naval Trusted AI projects administered by Crane NSWC, with an overarching focus of effort summarized in Figure¬†1. It will not rely upon pre-trained ‚Äúblack box‚Äù third party inference engines. This requires leveraging a smaller trusted training data set and capitalizing on both KG and NLP contributions to reach acceptable levels of accuracy. Additionally, we include visualizations and robustly explore the parameter tuning space.\n\n\n\n\n\n\nFigure¬†1: Conceptual Framework Principles for TrustedAI in the AI System Lifecycle\n\n\n\nWe propose a hybrid KG and NLP based solution for knowledge extraction from large volumes of textual mission/system data given relatively small volumes of labeled training articles (shown in Figure¬†2). Preliminary validation of this solution is being performed by ND faculty and students in collaboration with Crane NSWC on multiple large select data sets of particular importance to US Navy missions (where large represents a size impractical for rapid analysis by human readers). The framework and computational workflow are being developed in such a way that military data scientists can select appropriate components such as support vector machines (SVMs), Recurrent Neural Networks (RNNs) or bidirectional encoder representations from transformers (BERT) to classify a large corpus of text documents given a small quantity of training documents. These tools will be used to investigate automated KG construction and enrichment providing deeper context to NLP tools.\n\n\n\n\n\n\nFigure¬†2: Overview of Interconnected NLP, Knowledge Engineering System\n\n\n\nThis project also includes a major workforce development component. 10 undergraduates will participate in the project for at least one semester each. Students are US citizens and it is expected that multiple students will be ROTC cadets in good standing toward becoming future military officers. Students enhance both their AI and cyber skills through interface with the project scientists and professional software engineers. They: 1) learn and leverage machine learning tools, 2) experience the veracity, volume, variety, velocity and value of big data first hand and 3) learn and demonstrate best practices in software engineering.",
    "crumbs": [
      "Projects",
      "Knowledge Representation and Engineering",
      "Trusted AI - Natural Language Processing and Knowledge Graphs for Naval Systems Intelligence"
    ]
  },
  {
    "objectID": "Projects/Knowledge_Representation_and_Engineering/kg_nlp.html#technical-approach-and-justification",
    "href": "Projects/Knowledge_Representation_and_Engineering/kg_nlp.html#technical-approach-and-justification",
    "title": "Trusted AI - Natural Language Processing and Knowledge Graphs for Naval Systems Intelligence",
    "section": "",
    "text": "Identifying the complex causes of potential mission or weapon system failure (or success) and determining effective responses to preventing (or ensuring) such requires leveraging best in class machine learning techniques on rapidly growing, but often poorly structured, data. While the tools available for data science continue to evolve, there remain significant challenges for teams of decision makers trying to wrangle insight from the large and complex data accessible to them. The processes of reviewing, labeling, and classifying massive amounts of information takes extensive time, money, and human power. Fortunately, recent advances in natural language processing (NLP) and related machine learning tools such as knowledge graphs (KG) can be harnessed to gain insight and answer these critical questions. Growing availability of both open source and commercial NLP tools has made it easier for experimentation but also easier to use machine learning algorithms as ‚Äúblack box‚Äù tools in which data is blindly input and tool parameters tweaked to obtain the highest accuracy score. This ‚Äúblack box‚Äù approach introduces uncertainty (untrusted AI) which could lead to global inefficiencies at best and unexpected miss-classifications at worst. Our framework is being developed with the larger set of Naval Trusted AI projects administered by Crane NSWC, with an overarching focus of effort summarized in Figure¬†1. It will not rely upon pre-trained ‚Äúblack box‚Äù third party inference engines. This requires leveraging a smaller trusted training data set and capitalizing on both KG and NLP contributions to reach acceptable levels of accuracy. Additionally, we include visualizations and robustly explore the parameter tuning space.\n\n\n\n\n\n\nFigure¬†1: Conceptual Framework Principles for TrustedAI in the AI System Lifecycle\n\n\n\nWe propose a hybrid KG and NLP based solution for knowledge extraction from large volumes of textual mission/system data given relatively small volumes of labeled training articles (shown in Figure¬†2). Preliminary validation of this solution is being performed by ND faculty and students in collaboration with Crane NSWC on multiple large select data sets of particular importance to US Navy missions (where large represents a size impractical for rapid analysis by human readers). The framework and computational workflow are being developed in such a way that military data scientists can select appropriate components such as support vector machines (SVMs), Recurrent Neural Networks (RNNs) or bidirectional encoder representations from transformers (BERT) to classify a large corpus of text documents given a small quantity of training documents. These tools will be used to investigate automated KG construction and enrichment providing deeper context to NLP tools.\n\n\n\n\n\n\nFigure¬†2: Overview of Interconnected NLP, Knowledge Engineering System\n\n\n\nThis project also includes a major workforce development component. 10 undergraduates will participate in the project for at least one semester each. Students are US citizens and it is expected that multiple students will be ROTC cadets in good standing toward becoming future military officers. Students enhance both their AI and cyber skills through interface with the project scientists and professional software engineers. They: 1) learn and leverage machine learning tools, 2) experience the veracity, volume, variety, velocity and value of big data first hand and 3) learn and demonstrate best practices in software engineering.",
    "crumbs": [
      "Projects",
      "Knowledge Representation and Engineering",
      "Trusted AI - Natural Language Processing and Knowledge Graphs for Naval Systems Intelligence"
    ]
  },
  {
    "objectID": "Projects/Knowledge_Representation_and_Engineering/kg_nlp.html#dod-and-naval-relevance",
    "href": "Projects/Knowledge_Representation_and_Engineering/kg_nlp.html#dod-and-naval-relevance",
    "title": "Trusted AI - Natural Language Processing and Knowledge Graphs for Naval Systems Intelligence",
    "section": "2 DoD and Naval Relevance",
    "text": "2 DoD and Naval Relevance\nThere are an increasing number of federal mandates for national AI focus and effort, we reference just a small subset: 1) Improvement in Cybersecurity and Artificial Intelligence capabilities is repeatedly cited in the United States National Security Strategy (2017) (‚ÄúNational Security Strategy of the United States of America‚Äù 2017) and National Cyber Strategy (2018) (‚ÄúNational Cyber Strategy of the United States of America‚Äù 2018). 2) Executive Order 13859 (2019) (Executive Office of the President 2019) mandates ‚ÄúMaintaining American Leadership in Artificial Intelligence‚Äù. 3) The National Security Commision on AI (2021) champions investment in ‚Äúrobust and reliable AI‚Äù (Schmidt et al. 2021). 4) The 2021 US DoD memorandum on ‚ÄúImplementing Responsible Artificial Intelligence in the DoD‚Äù (Hicks 2021) directs our AI systems to be ethical and trusted. In recognition of this national priority and the growing foundation of data upon which military cyber operates, the US Navy has developed a framework for naval research and development that includes ‚ÄúInformation, Cyber and Spectrum Superiority‚Äù as a major focus area of their integrated research portfolio. For the US Navy and broader DoD community to meet these information and cyber research objectives, they must support the professional development of engineers, scientists and technical managers. Our project will help meet that challenge by advancing trusted machine learning knowledge as well as the skill sets of a broader workforce of (US citizen) students.\n\n2.1 ONR Relevance\nTrusted AI students are working with POCs at CRANE to develop trusted AI frameworks leveraging knowledge engineering built on Natural Language Processing and KG tools for insight and decision support relative to Naval weapon systems. This work aligns with the following Chief of Naval Research and ONR priority areas:\n\nChief of Naval Research Priority: Decision Superiority, Dominance in the Cognitive Domain\nONR Priority: Machine Learning, Reasoning and Intelligence\n\n\n\n2.2 CRANE NSWC Relevance\nNotre Dame and Trusted AI students are aligning their work on specific Naval use cases through NSWC Crane. The team is using Naval data with Naval questions for discovery paths. The team is working directly with NSWC Crane representatives for direct compatibility and transition planning as the research develops. This research aligns with NSWC Crane knowledge priorities as well as direct project needs of multiple groups.",
    "crumbs": [
      "Projects",
      "Knowledge Representation and Engineering",
      "Trusted AI - Natural Language Processing and Knowledge Graphs for Naval Systems Intelligence"
    ]
  },
  {
    "objectID": "Projects/Knowledge_Representation_and_Engineering/kg_nlp.html#naval-partnerships",
    "href": "Projects/Knowledge_Representation_and_Engineering/kg_nlp.html#naval-partnerships",
    "title": "Trusted AI - Natural Language Processing and Knowledge Graphs for Naval Systems Intelligence",
    "section": "3 Naval Partnerships",
    "text": "3 Naval Partnerships\nThe University of Notre Dame has multiple active funded research activities in partnership with Naval Surface Warfare Center Crane. Specifically, the lead PI has bi-weekly meetings with his NSWC Crane TPOCs on the large-scale multiple institution Trusted AI collaboration. Further, the University of Notre Dame partners with NSWC Crane, Indiana University and Purdue University through the DoD supported SCALE (Scalable Asymmetric Lifecycle Engagement) workforce development initiatives focused on microelectronics and machine learning research initiatives. The PIs also work with our colleagues at NSWC Crane, Office of Naval Research and Notre Dame Navy ROTC, to hold naval program and career ‚Äúworkshops‚Äù with each year‚Äôs cohort of students. Chief of Naval Research Rear Adm. Lorin C. Selby was our in person guest speaker during the fall of 2021. Specific to this TAI KG+NLP effort, we meet with our Crane NSWC TPOCs Alicia Scott and Eli Phillips at least bi-weekly and have additional deep dive discussions with Crane NSWC data providers and program managers. For our initial research effort we have been provided two naval maintenance log data sets; each with thousands of maintenance log records which include natural language text descriptions of each maintenance event. For both mission relevance and initial prototyping, we have identified three mission related questions that we seek to answer from the data in a trusted and automated manner with suitable accuracy:\n\n\nWhat correlations exist for failures in systems X and systems Y?\nIs the failure part listed in the categorical field the same that is identified in the text field? What should it be updated to if incorrect?\nIs failure X a part of subsystem W and/or system Q?",
    "crumbs": [
      "Projects",
      "Knowledge Representation and Engineering",
      "Trusted AI - Natural Language Processing and Knowledge Graphs for Naval Systems Intelligence"
    ]
  },
  {
    "objectID": "Projects/Knowledge_Representation_and_Engineering/kg_nlp.html#scientific-and-technical-progress",
    "href": "Projects/Knowledge_Representation_and_Engineering/kg_nlp.html#scientific-and-technical-progress",
    "title": "Trusted AI - Natural Language Processing and Knowledge Graphs for Naval Systems Intelligence",
    "section": "4 Scientific and Technical Progress",
    "text": "4 Scientific and Technical Progress\nPhase 1 (Y1+3M June 2021 ‚Äì September 2022) of the project delivered a hybrid KG and NLP based solution for knowledge extraction from large volumes of textual mission/system data given relatively small volumes of labeled training articles (and little to no external [untrusted] training data). Preliminary validation of this solution was performed by ND faculty and students in collaboration with Crane on multiple large maintenance data sets of particular importance to the USN (where large represents a size impractical for rapid analysis by human readers). The data science team evaluated appropriate components such as support vector machines (SVMs), Recurrent Neural Networks (RNNs) and bidirectional encoder representations from transformers (BERT) to classify a large corpus of maintenance logs given a small quantity of training documents. Given the limitations of a smaller trusted training set it was found that the SVMs provided the highest weapon subsystem classification accuracies ranging from 96% to 57% relative to the contextual differences in log event descriptions between the specific weapon subsystems. NN based classifiers leveraged pretrained open source nets such as the Hugging Face BERT NN with modestly lower accuracies than the SVMs. Given the pretrained external nets introduced additional risk (less trust) the SVMs were deemed a valid high trust option.\nFor further accuracy improvement, the team has shifted to its parallel work in KG creation and is enriching KGs with NLP based context aware tools to enrich KGs for higher accuracy weapon subsystem classification. A summary of the KG enrichment pipeline is shown in Figure¬†3.\n\n\n\n\n\n\nFigure¬†3: NLP Information Extraction Pipeline to Enhance KG Construction",
    "crumbs": [
      "Projects",
      "Knowledge Representation and Engineering",
      "Trusted AI - Natural Language Processing and Knowledge Graphs for Naval Systems Intelligence"
    ]
  }
]